# -*- coding: utf-8 -*-
"""Git_Credit_Card_Default.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1doQmrPAkVMK_TDJyrY89fUlZD2h-hK_F

<h1> Objective </h1>
The dataset consists of 10000 individuals and whether their credit card has defaulted or not. 
Predict whether the individual will default in their credit card payment.

# Loading DataSet
"""

#Download dataset
!wget -q https://www.dropbox.com/s/2ru004fjr6q13zm/default.csv

"""# Importing Modules"""

#Import basic packages

import warnings
warnings.filterwarnings("ignore")
import time
import pandas as pd               
import numpy as np
import pickle

from sklearn.model_selection import train_test_split   #splitting data
from pylab import rcParams
from sklearn.linear_model import LinearRegression         #linear regression
from sklearn.metrics.regression import mean_squared_error #error metrics
from sklearn.metrics import mean_absolute_error

import seaborn as sns                       #visualisation
import matplotlib.pyplot as plt             #visualisation

# %matplotlib inline     
sns.set(color_codes=True)

"""# Exploaratory Data Analysis

Reading Data
"""

# Read data through Pandas and compute time taken to read

t_start = time.time()
df_credit = pd.read_csv('default.csv')
t_end = time.time()
print('Time to Load Data: {} s'.format(t_end-t_start)) # time [s]
df_credit.head()

#Let's look into the total number of columns and observations in the dataset
df_credit.info()

#Let's look into summary statistics of data
df_credit.describe()

"""**Statistical Insight Using Pandas profiling**"""

#Perform Pandas profiling to understand quick overview of columns

import pandas_profiling
report = pandas_profiling.ProfileReport(df_credit)
#covert profile report as html file
report.to_file("credit_data.html")

from IPython.display import display,HTML,IFrame

display(HTML(open('credit_data.html').read()))

# count the number of NaN values in each column
print(df_credit.isnull().sum())

#The datatypes have now been changed
df_credit.info()

"""# Data Visualisation"""

# Relation between balance and default
# %matplotlib inline
sns.boxplot(x='default', y='balance', data=df_credit)
plt.show()

"""**Observation:**
- People who have more balance have defaulted more in their credit card payments.
"""

# Relation between income and default

sns.boxplot(x='default', y='income', data=df_credit)
plt.show()

"""**Observation:** 
- There is not much relation between income and whether they have defaulted in their credit card payments.
"""

#Install Packages

!pip -q install plotly-express

# Code for displaying plotly express plots inline in colab
def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))
  
import plotly_express as px

# Relation between balance and income and whether they have defaulted or not
configure_plotly_browser_state()
px.scatter(df_credit, x="income", y="balance" ,color="default",
           hover_data=["student"], log_x=True, size_max=60)

"""**Observation:**
- People who have more balance have more defaults.
"""

# Relation between Student and default

pd.crosstab(df_credit['default'], df_credit['student'], rownames=['Default'], colnames=['Student'])

# Correlation between selected variables
plt.figure(figsize=(5,5))
c = df_credit.corr()
sns.heatmap(c,cmap="BrBG",annot=True)

"""**Observation:**
- The heat map illustrates that income and balance are negatively correlated.
"""

# Explore how often a student defaults
configure_plotly_browser_state()
px.box(df_credit, x="default", y="income", color="student",hover_data=['balance'],notched=True)

"""**Observation:**

- Student defaulters are lesser as compared to others

# Model Building
"""

#Select the variables to be one-hot encoded
one_hot_features = ['student', 'default']
# Convert categorical variables into dummy/indicator variables (i.e. one-hot encoding).
one_hot_encoded = pd.get_dummies(df_credit[one_hot_features],drop_first=True)
one_hot_encoded.info(verbose=True, memory_usage=True, null_counts=True)

# Convert Categorical to Numerical for default column

# Replacing categorical columns with dummies
fdf = df_credit.drop(one_hot_features,axis=1)
fdf = pd.concat([fdf, one_hot_encoded] ,axis=1)
fdf.head()

fdf.info()

"""**Standardization**"""

#Standardize rows into uniform scale

X = fdf.drop(['default_Yes'],axis=1)
y = fdf['default_Yes']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)

# Scale and center the data
fdf_normalized = scaler.transform(X)

# Create a pandas DataFrame
fdf_normalized = pd.DataFrame(data=X, index=X.index, columns=X.columns)

"""**Split the data into train and test**"""

X_train, X_test, y_train, y_test = train_test_split(fdf_normalized,y,random_state=1234,test_size=0.3)

"""**Train the model**"""

# Building the Logistic Regression Model

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(C=1)                            # Set Large C value for low regularization to prevent overfitting
logreg.fit(X_train, y_train)

print(logreg.coef_)                                            # Coefficients for Logistic Regression
print(logreg.intercept_)

"""**Test the model**"""

# now applying our learnt model on test and also on train data

y_pred_test = logreg.predict(X_test)
y_pred_train = logreg.predict(X_train)

"""**Accuracy of the model**"""

from sklearn import metrics
# comparing the metrics of predicted lebel and real label of test data
print("Test Accuracy: ", metrics.accuracy_score(y_test, y_pred_test))

# comparing the metrics of predicted lebel and real label of test data
print("Train Accuracy: ", metrics.accuracy_score(y_train, y_pred_train))

"""**Null Accuracy**"""

# Actual Values of y_test
y_test.value_counts()
y_test.value_counts().head(1) / len(y_test)

"""The null accuracy is **96.67%**. There is only a slight improvement in accuracy for our model from the baseline model.

Accuracy is not an appropriate metric in this scenario. We employ confusion matrix and ROC curve to further understand performance of our model on test data.

**Confusion Matrix**
"""

conf=metrics.confusion_matrix(y_test, y_pred_test)

cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)
sns.heatmap(conf,cmap = cmap,xticklabels=['Prediction No','Prediction Yes'],yticklabels=['Actual No','Actual Yes'], annot=True,
            fmt='d')

"""**Observation:**
- Unfortunately, the model has not predicted any of the defaulters correctly.
"""

# Creating Classification Report

cr = metrics.classification_report(y_test, y_pred_test)
print(cr)

"""**Imbalanced Data**

The above metrics illustrate there is a clear imbalance in data.

Let us retrain our data using logistic regression this time using a parameter called 'balanced' which would handle imbalance in the data by changing the threshold set by logistic regression model:
"""

# Building the Logistic Regression Model

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(C=1e42,class_weight='balanced')                     # Set Large C value for low regularization to prevent overfitting
logreg.fit(X_train, y_train)

print(logreg.coef_)                                                             # Coefficients for Logistic Regression
print(logreg.intercept_)

"""**saveing the model**"""

from sklearn.externals import joblib 
  
# Save the model as a pickle in a file 
joblib.dump(logreg, 'model.pkl')

# now applying our learnt model on test and also on train data

y_pred_test = logreg.predict(X_test)
y_pred_train = logreg.predict(X_train)

from sklearn import metrics
# comparing the metrics of predicted lebel and real label of test data
print("Test Accuracy: ", metrics.accuracy_score(y_test, y_pred_test))

# creating a confusion matrix to understand the classification
conf = metrics.confusion_matrix(y_test, y_pred_test)
cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)
sns.heatmap(conf,cmap = cmap,xticklabels=['Prediction No','Prediction Yes'],yticklabels=['Actual No','Actual Yes'], annot=True,
            fmt='d')

# Creating Classification Report

cr = metrics.classification_report(y_test, y_pred_test)
print(cr)

"""**ROC Curve**"""

predict_probabilities = logreg.predict_proba(X_test)
fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_probabilities[:,1])

plt.plot(fpr, tpr)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.title('ROC curve for credit default')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.grid(True)
